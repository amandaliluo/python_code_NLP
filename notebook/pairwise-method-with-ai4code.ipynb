{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup #","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import accuracy_score\n\npd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\ndata_dir = Path('../input/AI4Code')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-30T02:30:01.629523Z","iopub.execute_input":"2022-06-30T02:30:01.631597Z","iopub.status.idle":"2022-06-30T02:30:01.642255Z","shell.execute_reply.started":"2022-06-30T02:30:01.631544Z","shell.execute_reply":"2022-06-30T02:30:01.641587Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"paths_train = list((data_dir / 'train').glob('*.json'))\nlen(paths_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:30:02.410614Z","iopub.execute_input":"2022-06-30T02:30:02.410893Z","iopub.status.idle":"2022-06-30T02:30:05.627813Z","shell.execute_reply.started":"2022-06-30T02:30:02.410862Z","shell.execute_reply":"2022-06-30T02:30:05.626997Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #","metadata":{}},{"cell_type":"markdown","source":"The notebooks are stored as individiual JSON files. They've been cleaned of the usual metadata present in Jupyter notebooks, leaving only the `cell_type` and `source`. The [Data](https://www.kaggle.com/competitions/AI4Code/data) page on the competition website has the full documentation of this dataset.\n\nWe'll load the notebooks here and join them into a dataframe for easier processing. The full set of training data takes quite a while to load, so we'll just use a subset for this demonstration.","metadata":{}},{"cell_type":"code","source":"NUM_TRAIN = 1000\n\n\ndef read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')\n    )\n\n\npaths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\nnotebooks_train = [\n    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n]\ndf = (\n    pd.concat(notebooks_train)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False)\n)\n\ndf","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-30T02:30:08.392140Z","iopub.execute_input":"2022-06-30T02:30:08.392425Z","iopub.status.idle":"2022-06-30T02:30:17.896547Z","shell.execute_reply.started":"2022-06-30T02:30:08.392395Z","shell.execute_reply":"2022-06-30T02:30:17.895646Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Ordering the Cells #","metadata":{}},{"cell_type":"markdown","source":"In the `train_orders.csv` file we have, for notebooks in the training set, the correct ordering of cells in terms of the cell ids.","metadata":{}},{"cell_type":"code","source":"df_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()  # Split the string representation of cell_ids into a list\n\ndf_orders","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-30T02:30:20.137336Z","iopub.execute_input":"2022-06-30T02:30:20.137593Z","iopub.status.idle":"2022-06-30T02:30:22.222658Z","shell.execute_reply.started":"2022-06-30T02:30:20.137564Z","shell.execute_reply":"2022-06-30T02:30:22.221950Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The correct numeric position of a cell we will call the **rank** of the cell. We can find the ranks of the cells within a notebook by referencing the true ordering of cell ids as given in `train_orders.csv`.","metadata":{}},{"cell_type":"code","source":"def get_dataset(df):\n    range_nbid = df.index.unique('id')\n    dict_pair = {}\n    for nb_id in tqdm(range_nbid):\n        nb = df.loc[nb_id, :]\n        cell_order = df_orders.loc[nb_id]\n\n        def get_ranks(base, derived):\n            return [base.index(d) for d in derived]\n\n        cell_ranks = get_ranks(cell_order, list(nb.index))\n        nb.insert(0, 'rank', cell_ranks)\n\n        nb_pos = nb.copy().reset_index()\n        rank_max = nb_pos['rank'].max()\n        nb_pos['cell_rank'] = np.minimum(nb_pos['rank']+1, rank_max)\n#         print(nb_pos.head())\n        nb_pos['cell_id_2'] = nb_pos['cell_id'][nb_pos['cell_rank']].values\n#         print(nb_pos.head())\n        nb_pos = nb_pos[nb_pos['cell_type']=='markdown']\n#         print(nb_pos.head())\n\n        nb_pos = nb_pos[nb_pos['cell_type']=='markdown'].reset_index()\n        nb_pos['label'] = 1\n#         print(nb_pos.head())\n        nb_pos = nb_pos[['cell_id','cell_id_2','label']]\n        nb_pos.columns = ['md_id','cell_id','label']\n#         print(nb_pos.head())\n\n        neg_ratio = 2\n\n        all_cell_rank = nb[nb['cell_type']!='markdown']['rank'].values\n        cell_ind = np.random.randint(0, len(all_cell_rank), len(nb_pos)*neg_ratio)\n        nb_rank = nb.copy().reset_index().set_index('rank')\n        nb_neg = nb_rank.iloc[list(cell_ind),].reset_index()\n        nb_neg['md_id'] = nb_pos.loc[nb_pos.index.repeat(neg_ratio)]['md_id'].values\n        nb_neg = nb_neg[['md_id','cell_id']]\n        nb_neg['label'] = 0\n\n        df_posneg = pd.concat([nb_pos, nb_neg]).reset_index(drop=True)\n\n        dict_pair[nb_id] = df_posneg.to_dict()\n        \n    df_pairs = pd.concat({k: pd.DataFrame(v) for k, v in dict_pair.items()})\n    df_pairs = df_pairs.drop_duplicates(subset=['md_id','cell_id'], keep='first')\n\n    return df_pairs","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:30:22.224136Z","iopub.execute_input":"2022-06-30T02:30:22.224763Z","iopub.status.idle":"2022-06-30T02:30:22.238119Z","shell.execute_reply.started":"2022-06-30T02:30:22.224705Z","shell.execute_reply":"2022-06-30T02:30:22.237302Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\ndf_ancestors\n\nfrom sklearn.model_selection import GroupShuffleSplit\n\nNVALID = 0.1  # size of validation set\n\nsplitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n\n# Split, keeping notebooks with a common origin (ancestor_id) together\nids = df.index.unique('id')\nancestors = df_ancestors.loc[ids, 'ancestor_id']\nids_train, ids_valid = next(splitter.split(ids, groups=ancestors))\nids_train, ids_valid = ids[ids_train], ids[ids_valid]\n\ndf_train = df.loc[ids_train, :]\ndf_valid = df.loc[ids_valid, :]\n\ndf_pairs_train = get_dataset(df_train)\ndf_pairs_valid = get_dataset(df_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:30:22.305198Z","iopub.execute_input":"2022-06-30T02:30:22.305503Z","iopub.status.idle":"2022-06-30T02:30:33.499323Z","shell.execute_reply.started":"2022-06-30T02:30:22.305474Z","shell.execute_reply":"2022-06-30T02:30:33.498131Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_pairs_train.tail(60)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:30:33.500833Z","iopub.execute_input":"2022-06-30T02:30:33.501027Z","iopub.status.idle":"2022-06-30T02:30:33.518360Z","shell.execute_reply.started":"2022-06-30T02:30:33.501002Z","shell.execute_reply":"2022-06-30T02:30:33.517712Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Sorting a notebook by the cell ranks is another way to order the notebook.","metadata":{}},{"cell_type":"code","source":"df_proc = df.copy().reset_index()\ndf_proc = df_proc[['cell_id','source']].set_index('cell_id')\ndf_proc","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:32.595611Z","iopub.execute_input":"2022-06-30T02:31:32.596107Z","iopub.status.idle":"2022-06-30T02:31:32.627354Z","shell.execute_reply.started":"2022-06-30T02:31:32.596060Z","shell.execute_reply":"2022-06-30T02:31:32.626577Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport sys, os\nfrom transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nMAX_LEN = 128\n\nclass MarkdownModel(nn.Module):\n    def __init__(self):\n        super(MarkdownModel, self).__init__()\n        self.distill_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.top = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask):\n        x = self.distill_bert(ids, mask)[0]\n        x = self.top(x[:, 0, :])\n        return x.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:33.603066Z","iopub.execute_input":"2022-06-30T02:31:33.603789Z","iopub.status.idle":"2022-06-30T02:31:40.955000Z","shell.execute_reply.started":"2022-06-30T02:31:33.603728Z","shell.execute_reply":"2022-06-30T02:31:40.954273Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 200","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:40.956513Z","iopub.execute_input":"2022-06-30T02:31:40.956776Z","iopub.status.idle":"2022-06-30T02:31:40.961647Z","shell.execute_reply.started":"2022-06-30T02:31:40.956723Z","shell.execute_reply":"2022-06-30T02:31:40.960181Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_pairs_train","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:40.962814Z","iopub.execute_input":"2022-06-30T02:31:40.963115Z","iopub.status.idle":"2022-06-30T02:31:40.982851Z","shell.execute_reply.started":"2022-06-30T02:31:40.963081Z","shell.execute_reply":"2022-06-30T02:31:40.981943Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_pairs_train.reset_index().iloc[100]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:40.984769Z","iopub.execute_input":"2022-06-30T02:31:40.985107Z","iopub.status.idle":"2022-06-30T02:31:41.000166Z","shell.execute_reply.started":"2022-06-30T02:31:40.984979Z","shell.execute_reply":"2022-06-30T02:31:40.999457Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass MarkdownDataset(Dataset):\n    \n    def __init__(self, df, df_source, max_len):\n        super().__init__()\n        self.df = df.reset_index()\n        self.df_source = df_source\n        self.max_len = max_len\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        nb = row['level_0']\n        md_idx = row['md_id']\n        md_source = self.df_source.loc[md_idx].values\n        code_idx = row['cell_id']\n        code_source = self.df_source.loc[code_idx].values\n        label = row['label']\n#         print(row)\n        \n        inputs_md = self.tokenizer.encode_plus(\n            (md_source+code_source)[0],\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = torch.LongTensor(inputs_md['input_ids'])\n        mask = torch.LongTensor(inputs_md['attention_mask'])\n\n        return [nb, md_idx, code_idx], ids, mask, label\n\n    def __len__(self):\n        return self.df.shape[0]\n    \ntrain_ds = MarkdownDataset(df_pairs_train, df_proc, max_len=MAX_LEN)\nval_ds = MarkdownDataset(df_pairs_valid, df_proc, max_len=MAX_LEN)\n\nval_ds[100]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:41.001606Z","iopub.execute_input":"2022-06-30T02:31:41.002253Z","iopub.status.idle":"2022-06-30T02:31:42.692267Z","shell.execute_reply.started":"2022-06-30T02:31:41.002216Z","shell.execute_reply":"2022-06-30T02:31:42.691591Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"BS = 16\nNW = 8\n\ntrain_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, num_workers=NW, pin_memory=False, drop_last=True)\nvalid_dl = DataLoader(val_ds, batch_size=BS, shuffle=False, num_workers=NW, pin_memory=False, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:42.693390Z","iopub.execute_input":"2022-06-30T02:31:42.695072Z","iopub.status.idle":"2022-06-30T02:31:42.702960Z","shell.execute_reply.started":"2022-06-30T02:31:42.695029Z","shell.execute_reply":"2022-06-30T02:31:42.702222Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def iterDataLoader(dataloader, model, lr, wd, train=False):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    ys = []\n    ys_pred = []\n    losses = []\n    pair_info = []\n    for pair_idx, idx, mask, y in tqdm(dataloader):\n        y_pred = model(idx.cuda(), mask.cuda()).float()\n        y = y.cuda().float()\n#         print(y, y_pred)\n        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n        \n        # option to update parameters\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step() \n        losses.append(loss.detach().cpu().item())\n        ys.append(y.detach().cpu().numpy())\n        ys_pred.append(y_pred.detach().cpu().numpy())\n        pair_info.append(pair_idx)\n\n    loss = np.mean(losses)\n    accu = accuracy_score(np.concatenate(ys), np.where(np.concatenate(ys_pred)> 0.5, 1, 0))\n    pairs_info = np.concatenate(pair_info)\n    return loss, accu, pairs_info","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:42.704193Z","iopub.execute_input":"2022-06-30T02:31:42.704658Z","iopub.status.idle":"2022-06-30T02:31:42.714467Z","shell.execute_reply.started":"2022-06-30T02:31:42.704622Z","shell.execute_reply":"2022-06-30T02:31:42.713793Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# training a model\ndef train_epocs(model, train_dl, valid_dl, n_epoch=5, lr=0.01, wd=0.0):\n    train_losses = []\n    train_accues = []\n    valid_losses = []\n    valid_accues = []\n    train_pairs = []\n    valid_pairs = []\n    for i in range(n_epoch):\n        model.train()\n        train_loss, train_accu, train_pair = iterDataLoader(train_dl, model, lr, wd, train=True)\n\n        train_losses.append(train_loss)\n        train_accues.append(train_accu)\n        train_pairs.extend(train_pair)\n\n        model.eval()\n        valid_loss, valid_accu, valid_pair = iterDataLoader(valid_dl, model, lr, wd, train=False)\n\n        valid_losses.append(valid_loss)\n        valid_accues.append(valid_accu)\n        valid_pairs.extend(valid_pair)\n\n        print(\"----- Epoch %.0f -----\\ntrain loss %.3f and valid loss %.3f, train accuracy %.3f and valid accuracy %.3f\"\\\n                 % (i+1, train_loss, valid_loss, train_accu, valid_accu)) \n\n    return train_losses, valid_losses, train_accues, valid_accues, train_pairs, valid_pairs","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:42.715740Z","iopub.execute_input":"2022-06-30T02:31:42.716016Z","iopub.status.idle":"2022-06-30T02:31:42.729993Z","shell.execute_reply.started":"2022-06-30T02:31:42.715983Z","shell.execute_reply":"2022-06-30T02:31:42.729244Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = MarkdownModel()\nmodel = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:31:42.731462Z","iopub.execute_input":"2022-06-30T02:31:42.731706Z","iopub.status.idle":"2022-06-30T02:32:02.243938Z","shell.execute_reply.started":"2022-06-30T02:31:42.731674Z","shell.execute_reply":"2022-06-30T02:32:02.243161Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ntrain_losses, valid_losses, train_accues, valid_accues, train_pairs, valid_pairs = train_epocs(model, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:32:02.247007Z","iopub.execute_input":"2022-06-30T02:32:02.247285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'mymodel.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"# model = TheModelClass(*args, **kwargs)\n# model.load_state_dict(torch.load(PATH))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:44.783886Z","iopub.execute_input":"2022-06-27T04:32:44.784155Z","iopub.status.idle":"2022-06-27T04:32:44.78877Z","shell.execute_reply.started":"2022-06-27T04:32:44.78412Z","shell.execute_reply":"2022-06-27T04:32:44.787871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths_train = list((data_dir / 'train').glob('*.json'))[10000:10001]\ninfer_df = [read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')]\ninfer_nb = (\n    pd.concat(infer_df)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False))\ninfer_nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_id = 'daf98d865b60d6'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_infer1 = infer_nb.reset_index()\nnb_infer2 = nb_infer1.copy()\nnb_infer1['key'] = 0\nnb_infer2['key'] = 0\n\nnb_infer = nb_infer1.merge(nb_infer2, on='key', how='outer')\nnb_infer = nb_infer[(nb_infer['cell_type_x']=='markdown')& (nb_infer['cell_type_y']=='code')]\nnb_infer['label'] = 999","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_infer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_infer_idx = nb_infer[['id_x','cell_id_x', 'cell_id_y', 'label']]\nnb_infer_idx.columns = ['level_0', 'md_id', 'cell_id','label']\nnb_infer_idx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_infer_proc = nb_infer1.copy().reset_index()\nnb_infer_proc = nb_infer_proc[['cell_id','source']].set_index('cell_id')\nnb_infer_proc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_ds = MarkdownDataset(nb_infer_idx, nb_infer_proc, max_len=MAX_LEN)\ninfer_ds[100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_dl = DataLoader(infer_ds, batch_size=32, shuffle=True, num_workers=NW, pin_memory=False, drop_last=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = []\npair_info = []\n\nmodel.eval()\nfor pair_idx, idx, mask, _ in tqdm(infer_dl):\n    y_pred = model(idx.cuda(), mask.cuda()).float()\n    y_preds.append(y_pred.detach().cpu().numpy())\n    pair_info.append(pair_idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pair_info_df = pd.DataFrame(np.concatenate(pair_info, axis=1)).T\npair_info_df['prob'] = F.sigmoid(torch.tensor(np.concatenate(y_preds)))\npair_info_df.columns = ['nb_id', 'md_id','code_id','prob']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = pair_info_df.groupby(['md_id'])['prob'].transform(max) == pair_info_df['prob']\npair_info_df[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}