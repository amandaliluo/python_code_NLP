{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup #","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import accuracy_score\n\npd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\ndata_dir = Path('../input/AI4Code')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T03:48:30.616593Z","iopub.execute_input":"2022-06-27T03:48:30.616892Z","iopub.status.idle":"2022-06-27T03:48:30.621972Z","shell.execute_reply.started":"2022-06-27T03:48:30.616863Z","shell.execute_reply":"2022-06-27T03:48:30.621257Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"paths_train = list((data_dir / 'train').glob('*.json'))\nlen(paths_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:31.024096Z","iopub.execute_input":"2022-06-27T03:48:31.024359Z","iopub.status.idle":"2022-06-27T03:48:31.717091Z","shell.execute_reply.started":"2022-06-27T03:48:31.024329Z","shell.execute_reply":"2022-06-27T03:48:31.716403Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #","metadata":{}},{"cell_type":"markdown","source":"The notebooks are stored as individiual JSON files. They've been cleaned of the usual metadata present in Jupyter notebooks, leaving only the `cell_type` and `source`. The [Data](https://www.kaggle.com/competitions/AI4Code/data) page on the competition website has the full documentation of this dataset.\n\nWe'll load the notebooks here and join them into a dataframe for easier processing. The full set of training data takes quite a while to load, so we'll just use a subset for this demonstration.","metadata":{}},{"cell_type":"code","source":"NUM_TRAIN = 1000\n\n\ndef read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')\n    )\n\n\npaths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\nnotebooks_train = [\n    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n]\ndf = (\n    pd.concat(notebooks_train)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False)\n)\n\ndf","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T03:48:31.926854Z","iopub.execute_input":"2022-06-27T03:48:31.927575Z","iopub.status.idle":"2022-06-27T03:48:40.562203Z","shell.execute_reply.started":"2022-06-27T03:48:31.927527Z","shell.execute_reply":"2022-06-27T03:48:40.561456Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Ordering the Cells #","metadata":{}},{"cell_type":"markdown","source":"In the `train_orders.csv` file we have, for notebooks in the training set, the correct ordering of cells in terms of the cell ids.","metadata":{}},{"cell_type":"code","source":"df_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()  # Split the string representation of cell_ids into a list\n\ndf_orders","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-06-27T03:48:40.564044Z","iopub.execute_input":"2022-06-27T03:48:40.564341Z","iopub.status.idle":"2022-06-27T03:48:42.874568Z","shell.execute_reply.started":"2022-06-27T03:48:40.564305Z","shell.execute_reply":"2022-06-27T03:48:42.873683Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The correct numeric position of a cell we will call the **rank** of the cell. We can find the ranks of the cells within a notebook by referencing the true ordering of cell ids as given in `train_orders.csv`.","metadata":{}},{"cell_type":"code","source":"def get_dataset(df):\n    range_nbid = df.index.unique('id')\n    dict_pair = {}\n    for nb_id in tqdm(range_nbid):\n        nb = df.loc[nb_id, :]\n        cell_order = df_orders.loc[nb_id]\n\n        def get_ranks(base, derived):\n            return [base.index(d) for d in derived]\n\n        cell_ranks = get_ranks(cell_order, list(nb.index))\n        nb.insert(0, 'rank', cell_ranks)\n\n        nb_pos = nb.copy().reset_index()\n        nb_pos['cell_rank'] = np.where(nb_pos['rank']+1<=nb_pos['rank'].max(), nb_pos['rank']+1, nb_pos['rank'].max())\n        nb_pos['cell_id_2'] = nb_pos['cell_id'][nb_pos['cell_rank']].values\n        nb_pos = nb_pos[nb_pos['cell_type']=='markdown']\n\n        nb_pos = nb_pos[nb_pos['cell_type']=='markdown'].reset_index()\n        nb_pos['label'] = 1\n        nb_pos = nb_pos[['cell_id','cell_id','label']]\n        nb_pos.columns = ['md_id','cell_id','label']\n\n        neg_ratio = 2\n\n        all_cell_rank = nb[nb['cell_type']!='markdown']['rank'].values\n        cell_ind = np.random.randint(0, len(all_cell_rank), len(nb_pos)*neg_ratio)\n        nb_rank = nb.copy().reset_index().set_index('rank')\n        nb_neg = nb_rank.iloc[list(cell_ind),].reset_index()\n        nb_neg['md_id'] = nb_pos.loc[nb_pos.index.repeat(neg_ratio)]['md_id'].values\n        nb_neg = nb_neg[['md_id','cell_id']]\n        nb_neg['label'] = 0\n\n        df_posneg = pd.concat([nb_pos, nb_neg]).reset_index(drop=True)\n\n        dict_pair[nb_id] = df_posneg.to_dict()\n        \n    df_pairs = pd.concat({k: pd.DataFrame(v) for k, v in dict_pair.items()})\n    df_pairs = df_pairs.drop_duplicates(subset=['md_id','cell_id'], keep='first')\n\n    return df_pairs","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:42.876104Z","iopub.execute_input":"2022-06-27T03:48:42.876369Z","iopub.status.idle":"2022-06-27T03:48:42.890077Z","shell.execute_reply.started":"2022-06-27T03:48:42.876333Z","shell.execute_reply":"2022-06-27T03:48:42.889081Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\ndf_ancestors\n\nfrom sklearn.model_selection import GroupShuffleSplit\n\nNVALID = 0.1  # size of validation set\n\nsplitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)\n\n# Split, keeping notebooks with a common origin (ancestor_id) together\nids = df.index.unique('id')\nancestors = df_ancestors.loc[ids, 'ancestor_id']\nids_train, ids_valid = next(splitter.split(ids, groups=ancestors))\nids_train, ids_valid = ids[ids_train], ids[ids_valid]\n\ndf_train = df.loc[ids_train, :]\ndf_valid = df.loc[ids_valid, :]\n\ndf_pairs_train = get_dataset(df_train)\ndf_pairs_valid = get_dataset(df_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:42.892729Z","iopub.execute_input":"2022-06-27T03:48:42.893034Z","iopub.status.idle":"2022-06-27T03:48:54.660831Z","shell.execute_reply.started":"2022-06-27T03:48:42.892994Z","shell.execute_reply":"2022-06-27T03:48:54.660076Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Sorting a notebook by the cell ranks is another way to order the notebook.","metadata":{}},{"cell_type":"code","source":"df_proc = df.copy().reset_index()\ndf_proc = df_proc[['cell_id','source']].set_index('cell_id')\ndf_proc","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:54.661973Z","iopub.execute_input":"2022-06-27T03:48:54.662240Z","iopub.status.idle":"2022-06-27T03:48:54.691331Z","shell.execute_reply.started":"2022-06-27T03:48:54.662204Z","shell.execute_reply":"2022-06-27T03:48:54.690648Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport sys, os\nfrom transformers import DistilBertModel, DistilBertTokenizer\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\nMAX_LEN = 128\n\nclass MarkdownModel(nn.Module):\n    def __init__(self):\n        super(MarkdownModel, self).__init__()\n        self.distill_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.top = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask):\n        x = self.distill_bert(ids, mask)[0]\n        x = self.top(x[:, 0, :])\n        return x.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:48:54.692760Z","iopub.execute_input":"2022-06-27T03:48:54.693013Z","iopub.status.idle":"2022-06-27T03:49:00.257796Z","shell.execute_reply.started":"2022-06-27T03:48:54.692979Z","shell.execute_reply":"2022-06-27T03:49:00.257074Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 200","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:00.259202Z","iopub.execute_input":"2022-06-27T03:49:00.259469Z","iopub.status.idle":"2022-06-27T03:49:00.267214Z","shell.execute_reply.started":"2022-06-27T03:49:00.259431Z","shell.execute_reply":"2022-06-27T03:49:00.266499Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_pairs_train","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:00.270371Z","iopub.execute_input":"2022-06-27T03:49:00.270558Z","iopub.status.idle":"2022-06-27T03:49:00.289367Z","shell.execute_reply.started":"2022-06-27T03:49:00.270534Z","shell.execute_reply":"2022-06-27T03:49:00.288647Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_pairs_train.reset_index().iloc[100]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:00.290748Z","iopub.execute_input":"2022-06-27T03:49:00.292287Z","iopub.status.idle":"2022-06-27T03:49:00.304964Z","shell.execute_reply.started":"2022-06-27T03:49:00.292258Z","shell.execute_reply":"2022-06-27T03:49:00.304310Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass MarkdownDataset(Dataset):\n    \n    def __init__(self, df, df_source, max_len):\n        super().__init__()\n        self.df = df.reset_index()\n        self.df_source = df_source\n        self.max_len = max_len\n        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        nb = row['level_0']\n        md_idx = row['md_id']\n        md_source = self.df_source.loc[md_idx].values\n        code_idx = row['cell_id']\n        code_source = self.df_source.loc[code_idx].values\n        label = row['label']\n#         print(row)\n        \n        inputs_md = self.tokenizer.encode_plus(\n            (md_source+code_source)[0],\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = torch.LongTensor(inputs_md['input_ids'])\n        mask = torch.LongTensor(inputs_md['attention_mask'])\n\n        return [nb, md_idx, code_idx], ids, mask, label\n\n    def __len__(self):\n        return self.df.shape[0]\n    \ntrain_ds = MarkdownDataset(df_pairs_train, df_proc, max_len=MAX_LEN)\nval_ds = MarkdownDataset(df_pairs_valid, df_proc, max_len=MAX_LEN)\n\nval_ds[100]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:00.307983Z","iopub.execute_input":"2022-06-27T03:49:00.308246Z","iopub.status.idle":"2022-06-27T03:49:05.137212Z","shell.execute_reply.started":"2022-06-27T03:49:00.308219Z","shell.execute_reply":"2022-06-27T03:49:05.136500Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"BS = 16\nNW = 8\n\ntrain_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, num_workers=NW, pin_memory=False, drop_last=True)\nvalid_dl = DataLoader(val_ds, batch_size=BS, shuffle=False, num_workers=NW, pin_memory=False, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:05.138623Z","iopub.execute_input":"2022-06-27T03:49:05.138915Z","iopub.status.idle":"2022-06-27T03:49:05.146589Z","shell.execute_reply.started":"2022-06-27T03:49:05.138880Z","shell.execute_reply":"2022-06-27T03:49:05.145664Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def iterDataLoader(dataloader, model, lr, wd, train=False):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    ys = []\n    ys_pred = []\n    losses = []\n    pair_info = []\n    for pair_idx, idx, mask, y in tqdm(dataloader):\n        y_pred = model(idx.cuda(), mask.cuda()).float()\n        y = y.cuda().float()\n#         print(y, y_pred)\n        loss = F.binary_cross_entropy_with_logits(y_pred, y)\n        \n        # option to update parameters\n        if train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step() \n        losses.append(loss.detach().cpu().item())\n        ys.append(y.detach().cpu().numpy())\n        ys_pred.append(y_pred.detach().cpu().numpy())\n        pair_info.append(pair_idx)\n\n    loss = np.mean(losses)\n    accu = accuracy_score(np.concatenate(ys), np.where(np.concatenate(ys_pred)> 0.5, 1, 0))\n    pairs_info = np.concatenate(pair_info)\n    return loss, accu, pairs_info","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:05.148033Z","iopub.execute_input":"2022-06-27T03:49:05.148308Z","iopub.status.idle":"2022-06-27T03:49:05.158777Z","shell.execute_reply.started":"2022-06-27T03:49:05.148270Z","shell.execute_reply":"2022-06-27T03:49:05.157929Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# training a model\ndef train_epocs(model, train_dl, valid_dl, n_epoch=5, lr=0.01, wd=0.0):\n    train_losses = []\n    train_accues = []\n    valid_losses = []\n    valid_accues = []\n    train_pairs = []\n    valid_pairs = []\n    for i in range(n_epoch):\n        model.train()\n        train_loss, train_accu, train_pair = iterDataLoader(train_dl, model, lr, wd, train=True)\n\n        train_losses.append(train_loss)\n        train_accues.append(train_accu)\n        train_pairs.extend(train_pair)\n\n        model.eval()\n        valid_loss, valid_accu, valid_pair = iterDataLoader(valid_dl, model, lr, wd, train=False)\n\n        valid_losses.append(valid_loss)\n        valid_accues.append(valid_accu)\n        valid_pairs.extend(valid_pair)\n\n        print(\"----- Epoch %.0f -----\\ntrain loss %.3f and valid loss %.3f, train accuracy %.3f and valid accuracy %.3f\"\\\n                 % (i+1, train_loss, valid_loss, train_accu, valid_accu)) \n\n    return train_losses, valid_losses, train_accues, valid_accues, train_pairs, valid_pairs","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:05.160481Z","iopub.execute_input":"2022-06-27T03:49:05.161007Z","iopub.status.idle":"2022-06-27T03:49:05.170977Z","shell.execute_reply.started":"2022-06-27T03:49:05.160970Z","shell.execute_reply":"2022-06-27T03:49:05.170089Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = MarkdownModel()\nmodel = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:05.173735Z","iopub.execute_input":"2022-06-27T03:49:05.174371Z","iopub.status.idle":"2022-06-27T03:49:17.710383Z","shell.execute_reply.started":"2022-06-27T03:49:05.174275Z","shell.execute_reply":"2022-06-27T03:49:17.709659Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ntrain_losses, valid_losses, train_accues, valid_accues, train_pairs, valid_pairs = train_epocs(model, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T03:49:17.711743Z","iopub.execute_input":"2022-06-27T03:49:17.711976Z","iopub.status.idle":"2022-06-27T04:32:44.272547Z","shell.execute_reply.started":"2022-06-27T03:49:17.711945Z","shell.execute_reply":"2022-06-27T04:32:44.271758Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'mymodel.pt')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:44.274234Z","iopub.execute_input":"2022-06-27T04:32:44.274712Z","iopub.status.idle":"2022-06-27T04:32:44.782380Z","shell.execute_reply.started":"2022-06-27T04:32:44.274670Z","shell.execute_reply":"2022-06-27T04:32:44.781656Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# inference","metadata":{}},{"cell_type":"code","source":"# model = TheModelClass(*args, **kwargs)\n# model.load_state_dict(torch.load(PATH))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:44.783886Z","iopub.execute_input":"2022-06-27T04:32:44.784155Z","iopub.status.idle":"2022-06-27T04:32:44.788770Z","shell.execute_reply.started":"2022-06-27T04:32:44.784120Z","shell.execute_reply":"2022-06-27T04:32:44.787871Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"paths_train = list((data_dir / 'train').glob('*.json'))[10000:10001]\ninfer_df = [read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')]\ninfer_nb = (\n    pd.concat(infer_df)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False))\ninfer_nb","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:44.790168Z","iopub.execute_input":"2022-06-27T04:32:44.790670Z","iopub.status.idle":"2022-06-27T04:32:45.837388Z","shell.execute_reply.started":"2022-06-27T04:32:44.790632Z","shell.execute_reply":"2022-06-27T04:32:45.836553Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"nb_id = 'daf98d865b60d6'","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.841249Z","iopub.execute_input":"2022-06-27T04:32:45.841510Z","iopub.status.idle":"2022-06-27T04:32:45.849889Z","shell.execute_reply.started":"2022-06-27T04:32:45.841475Z","shell.execute_reply":"2022-06-27T04:32:45.848661Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"nb_infer1 = infer_nb.reset_index()\nnb_infer2 = nb_infer1.copy()\nnb_infer1['key'] = 0\nnb_infer2['key'] = 0\n\nnb_infer = nb_infer1.merge(nb_infer2, on='key', how='outer')\nnb_infer = nb_infer[(nb_infer['cell_type_x']=='markdown')& (nb_infer['cell_type_y']=='code')]\nnb_infer['label'] = 999","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.855023Z","iopub.execute_input":"2022-06-27T04:32:45.855270Z","iopub.status.idle":"2022-06-27T04:32:45.906940Z","shell.execute_reply.started":"2022-06-27T04:32:45.855238Z","shell.execute_reply":"2022-06-27T04:32:45.906092Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"nb_infer","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.910965Z","iopub.execute_input":"2022-06-27T04:32:45.911217Z","iopub.status.idle":"2022-06-27T04:32:45.940906Z","shell.execute_reply.started":"2022-06-27T04:32:45.911184Z","shell.execute_reply":"2022-06-27T04:32:45.940113Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"nb_infer_idx = nb_infer[['id_x','cell_id_x', 'cell_id_y', 'label']]\nnb_infer_idx.columns = ['level_0', 'md_id', 'cell_id','label']\nnb_infer_idx","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.944808Z","iopub.execute_input":"2022-06-27T04:32:45.945073Z","iopub.status.idle":"2022-06-27T04:32:45.969464Z","shell.execute_reply.started":"2022-06-27T04:32:45.945032Z","shell.execute_reply":"2022-06-27T04:32:45.968662Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"nb_infer_proc = nb_infer1.copy().reset_index()\nnb_infer_proc = nb_infer_proc[['cell_id','source']].set_index('cell_id')\nnb_infer_proc","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.973271Z","iopub.execute_input":"2022-06-27T04:32:45.973536Z","iopub.status.idle":"2022-06-27T04:32:45.995140Z","shell.execute_reply.started":"2022-06-27T04:32:45.973501Z","shell.execute_reply":"2022-06-27T04:32:45.994484Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"infer_ds = MarkdownDataset(nb_infer_idx, nb_infer_proc, max_len=MAX_LEN)\ninfer_ds[100]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:45.996158Z","iopub.execute_input":"2022-06-27T04:32:45.996524Z","iopub.status.idle":"2022-06-27T04:32:47.711182Z","shell.execute_reply.started":"2022-06-27T04:32:45.996489Z","shell.execute_reply":"2022-06-27T04:32:47.710497Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"infer_dl = DataLoader(infer_ds, batch_size=32, shuffle=True, num_workers=NW, pin_memory=False, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:47.712308Z","iopub.execute_input":"2022-06-27T04:32:47.713076Z","iopub.status.idle":"2022-06-27T04:32:47.718544Z","shell.execute_reply.started":"2022-06-27T04:32:47.713027Z","shell.execute_reply":"2022-06-27T04:32:47.717636Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# model = model.cpu()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:47.720083Z","iopub.execute_input":"2022-06-27T04:32:47.720438Z","iopub.status.idle":"2022-06-27T04:32:47.727801Z","shell.execute_reply.started":"2022-06-27T04:32:47.720399Z","shell.execute_reply":"2022-06-27T04:32:47.726976Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"y_preds = []\npair_info = []\n\nmodel.eval()\nfor pair_idx, idx, mask, _ in tqdm(infer_dl):\n    y_pred = model(idx.cuda(), mask.cuda()).float()\n    y_preds.append(y_pred.detach().cpu().numpy())\n    pair_info.append(pair_idx)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:32:47.729047Z","iopub.execute_input":"2022-06-27T04:32:47.729663Z","iopub.status.idle":"2022-06-27T04:33:26.999357Z","shell.execute_reply.started":"2022-06-27T04:32:47.729619Z","shell.execute_reply":"2022-06-27T04:33:26.998437Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"pair_info_df = pd.DataFrame(np.concatenate(pair_info, axis=1)).T\npair_info_df['prob'] = F.sigmoid(torch.tensor(np.concatenate(y_preds)))\npair_info_df.columns = ['nb_id', 'md_id','code_id','prob']","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:33:27.003653Z","iopub.execute_input":"2022-06-27T04:33:27.004010Z","iopub.status.idle":"2022-06-27T04:33:27.040108Z","shell.execute_reply.started":"2022-06-27T04:33:27.003980Z","shell.execute_reply":"2022-06-27T04:33:27.039284Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"idx = pair_info_df.groupby(['md_id'])['prob'].transform(max) == pair_info_df['prob']\npair_info_df[idx]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T04:33:27.041709Z","iopub.execute_input":"2022-06-27T04:33:27.041970Z","iopub.status.idle":"2022-06-27T04:33:27.064441Z","shell.execute_reply.started":"2022-06-27T04:33:27.041935Z","shell.execute_reply":"2022-06-27T04:33:27.063545Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}