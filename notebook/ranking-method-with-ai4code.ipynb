{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\n# let's get an idea of word frequency\nfrom collections import Counter\n\n# tool for text\nimport spacy\nimport re\n\nfrom pandas.testing import assert_frame_equal\n\npd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\ndata_dir = Path('../input/AI4Code')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:17.442635Z","iopub.execute_input":"2022-06-29T22:57:17.443440Z","iopub.status.idle":"2022-06-29T22:57:20.141745Z","shell.execute_reply.started":"2022-06-29T22:57:17.443325Z","shell.execute_reply":"2022-06-29T22:57:20.140585Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:20.144006Z","iopub.execute_input":"2022-06-29T22:57:20.144733Z","iopub.status.idle":"2022-06-29T22:57:20.151216Z","shell.execute_reply.started":"2022-06-29T22:57:20.144685Z","shell.execute_reply":"2022-06-29T22:57:20.150174Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# list of all the paths for all the training data\npaths_train = list((data_dir / 'train').glob('*.json'))\n\n# a list of all the notebook ids\nnb_ids = [str(path).split('/')[-1].split('.')[0] for path in paths_train]\n\n# create a df of the path and the notebook_id\ntraining_dict = {'path': paths_train, 'nb_id': nb_ids}\ntraining_paths_df = pd.DataFrame.from_dict(training_dict)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:20.152738Z","iopub.execute_input":"2022-06-29T22:57:20.153036Z","iopub.status.idle":"2022-06-29T22:57:24.096257Z","shell.execute_reply.started":"2022-06-29T22:57:20.153008Z","shell.execute_reply":"2022-06-29T22:57:24.094831Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# getting the correct order of the cells\ndf_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:24.099450Z","iopub.execute_input":"2022-06-29T22:57:24.099954Z","iopub.status.idle":"2022-06-29T22:57:27.290140Z","shell.execute_reply.started":"2022-06-29T22:57:24.099903Z","shell.execute_reply":"2022-06-29T22:57:27.289139Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_ranks(base, derived):\n    return [base.index(d) for d in derived]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:27.291336Z","iopub.execute_input":"2022-06-29T22:57:27.291702Z","iopub.status.idle":"2022-06-29T22:57:27.296717Z","shell.execute_reply.started":"2022-06-29T22:57:27.291671Z","shell.execute_reply":"2022-06-29T22:57:27.295932Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PythonDataset(Dataset):\n    def __init__(self, df, df_orders):\n        self.df = df\n        self.df_orders = df_orders\n\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = training_paths_df.iloc[idx]\n        # retriveing a singel file and converting it to a dataframe\n        disorganized_df = read_notebook(row['path'])\n        cell_order = df_orders[row['nb_id']]\n        \n        # rank each of the cell in that specific notebook\n        cell_ranks = get_ranks(cell_order, list(disorganized_df.index))\n        # insert the ranks back into the dataframe\n        disorganized_df.insert(0, 'rank', cell_ranks)\n\n        organized_df = disorganized_df.copy()[['rank', 'cell_type', 'source']]\n        organized_df['ranked_cleaned'] = np.where(\n                                            organized_df['cell_type'] == 'code',\n                                            organized_df.groupby(['cell_type']).cumcount().to_numpy() + 1,\n                                            0,)\n        return organized_df","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:27.297848Z","iopub.execute_input":"2022-06-29T22:57:27.298632Z","iopub.status.idle":"2022-06-29T22:57:27.310869Z","shell.execute_reply.started":"2022-06-29T22:57:27.298598Z","shell.execute_reply":"2022-06-29T22:57:27.309571Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Split Data","metadata":{}},{"cell_type":"code","source":"whole_train = training_paths_df.iloc[:1000, :].copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:27.312081Z","iopub.execute_input":"2022-06-29T22:57:27.312856Z","iopub.status.idle":"2022-06-29T22:57:27.327941Z","shell.execute_reply.started":"2022-06-29T22:57:27.312825Z","shell.execute_reply":"2022-06-29T22:57:27.326952Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"whole_train_ds = PythonDataset(whole_train, df_orders)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:27.331343Z","iopub.execute_input":"2022-06-29T22:57:27.332814Z","iopub.status.idle":"2022-06-29T22:57:27.340016Z","shell.execute_reply.started":"2022-06-29T22:57:27.332758Z","shell.execute_reply":"2022-06-29T22:57:27.339144Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"nlp= spacy.load('en_core_web_sm', disable = ['ner', 'parser'])\ndef clean_text(df, nlp, column):\n    rows = []\n    for idx in range(len(df)):\n        row = df.iloc[idx].copy()\n\n        # first we remove numeric characters and lowercase everything\n        cleaned_review = re.sub(\"[^A-Za-z']+\", ' ', row[column].replace('<br />', ' ')).lower()\n        # we let spaCy tokenize the text for us\n        tokenized_review = nlp(cleaned_review)\n        cleaned_tokenized = [token.lemma_ for token in tokenized_review]\n        cleaned_tokenized = [token for token in cleaned_tokenized if len(token)>1]\n\n        if len(cleaned_tokenized) >= 1:\n            row['cleaned'] = ' '.join(cleaned_tokenized)\n\n        rows.append(row)\n    data = pd.DataFrame(rows)\n    data = data.reset_index()\n    idx_nans = np.where(data['cleaned'].isna())[0]\n    data = data.drop(idx_nans)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:27.341193Z","iopub.execute_input":"2022-06-29T22:57:27.342152Z","iopub.status.idle":"2022-06-29T22:57:28.473913Z","shell.execute_reply.started":"2022-06-29T22:57:27.342118Z","shell.execute_reply":"2022-06-29T22:57:28.472762Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def find_max_length(data):\n    max_length = 0\n    for i in range(len(data)):\n        row = data.iloc[i]['cleaned']\n        length = len(row.split())\n\n        if length > max_length:\n            max_length = length\n    return max_length","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:28.477934Z","iopub.execute_input":"2022-06-29T22:57:28.478279Z","iopub.status.idle":"2022-06-29T22:57:28.484643Z","shell.execute_reply.started":"2022-06-29T22:57:28.478249Z","shell.execute_reply":"2022-06-29T22:57:28.483544Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_whole_df(df, df_ds):\n    final_df = None\n\n    for i in tqdm(range(len(df))):\n        temp_df = df_ds[i]\n        cleaned_df = clean_text(temp_df, nlp, 'source')\n\n        if i == 0:\n            final_df = cleaned_df\n        final_df = pd.concat([final_df, cleaned_df])\n\n    return final_df","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:28.486194Z","iopub.execute_input":"2022-06-29T22:57:28.486621Z","iopub.status.idle":"2022-06-29T22:57:28.497540Z","shell.execute_reply.started":"2022-06-29T22:57:28.486580Z","shell.execute_reply":"2022-06-29T22:57:28.496451Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def make_word_dict(cleaned_df):\n    sentences = [string.split(' ') for string in list(cleaned_df['cleaned'])]\n    word_freq = Counter([token for string in sentences for token in string]).most_common()\n    word_freq_dict = dict(word_freq)\n    \n    min_freq = 5\n    word_dict = {}\n\n    # sending all the unknowns to 0\n    i = 1\n    for word in word_freq_dict:\n        if word_freq_dict[word] > min_freq:\n            word_dict[word] = i\n            i += 1\n        else:\n            word_dict[word] = 0\n    \n    # dictionary length        \n    dict_length = max(word_dict.values()) + 1\n    \n    return dict_length, word_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:28.499225Z","iopub.execute_input":"2022-06-29T22:57:28.499912Z","iopub.status.idle":"2022-06-29T22:57:28.510317Z","shell.execute_reply.started":"2022-06-29T22:57:28.499875Z","shell.execute_reply":"2022-06-29T22:57:28.509519Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"final_train = preprocess_whole_df(whole_train, whole_train_ds)\ndict_length, word_dict = make_word_dict(final_train)\nmax_length = find_max_length(final_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:57:28.512456Z","iopub.execute_input":"2022-06-29T22:57:28.513444Z","iopub.status.idle":"2022-06-29T23:01:49.126771Z","shell.execute_reply.started":"2022-06-29T22:57:28.513393Z","shell.execute_reply":"2022-06-29T23:01:49.125684Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class PythonDataset2(Dataset):\n    def __init__(self, df, word_dict, max_length):\n        self.df = df\n        self.word_dict = word_dict\n        self.max_len = max_length\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n#         print(row)\n        content = row['cleaned'].split(' ')\n        \n        # find the idx that is asscoiated with the particular word\n        content_idxs = [self.word_dict[word] for word in content]\n        # front pad the sentence \n        cleaned_content_arr = np.array(content_idxs)\n        zeros_arr = np.zeros(max_length - len(content))\n        padded_arr = np.concatenate([zeros_arr, cleaned_content_arr])\n        \n        x = torch.LongTensor(padded_arr) \n        x2 = torch.tensor(row['ranked_cleaned']).float()\n        y = torch.tensor(row['rank']).float()\n        \n        # embedding likes long tensors\n        return x, x2, y","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.128638Z","iopub.execute_input":"2022-06-29T23:01:49.129432Z","iopub.status.idle":"2022-06-29T23:01:49.140452Z","shell.execute_reply.started":"2022-06-29T23:01:49.129366Z","shell.execute_reply":"2022-06-29T23:01:49.139443Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"N = int(final_train.shape[0] * 0.8)\ntrain = final_train.iloc[:N, :]\nval = final_train.iloc[N:, :]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.142127Z","iopub.execute_input":"2022-06-29T23:01:49.142901Z","iopub.status.idle":"2022-06-29T23:01:49.154281Z","shell.execute_reply.started":"2022-06-29T23:01:49.142864Z","shell.execute_reply":"2022-06-29T23:01:49.153445Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train.shape, val.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.155787Z","iopub.execute_input":"2022-06-29T23:01:49.156241Z","iopub.status.idle":"2022-06-29T23:01:49.170407Z","shell.execute_reply.started":"2022-06-29T23:01:49.156210Z","shell.execute_reply":"2022-06-29T23:01:49.169205Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_ds = PythonDataset2(train, word_dict, max_length)\nval_ds = PythonDataset2(val, word_dict, max_length)\n\ntrain_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\nval_dl = DataLoader(train_ds, batch_size=10, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.171767Z","iopub.execute_input":"2022-06-29T23:01:49.172105Z","iopub.status.idle":"2022-06-29T23:01:49.181930Z","shell.execute_reply.started":"2022-06-29T23:01:49.172077Z","shell.execute_reply":"2022-06-29T23:01:49.180774Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.183707Z","iopub.execute_input":"2022-06-29T23:01:49.184344Z","iopub.status.idle":"2022-06-29T23:01:49.197821Z","shell.execute_reply.started":"2022-06-29T23:01:49.184311Z","shell.execute_reply":"2022-06-29T23:01:49.196899Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Vanilla RNN using nn.RNN\nclass RNN(nn.Module):\n    def __init__(self, dict_length, max_length, emb_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        # embed the words\n        self.emb = nn.Embedding(dict_length, emb_size, padding_idx=0)  \n        \n        self.linear2 = nn.Linear((max_length*emb_size)+1, output_size)\n\n    def forward(self, x, x2):\n        \n        # RNN layer outputs a tuple, the output and the final hidden state\n        # taking the final hidden state as output\n#         print('-------', x.size(), max_length)\n        x = self.emb(x)\n        x = x.view(x.shape[0],x.shape[1]*x.shape[-1])\n\n        x2 = x2.unsqueeze(1)\n        X = torch.cat((x,x2), 1)\n        out = self.linear2(X)\n        return out.squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.199640Z","iopub.execute_input":"2022-06-29T23:01:49.200662Z","iopub.status.idle":"2022-06-29T23:01:49.209457Z","shell.execute_reply.started":"2022-06-29T23:01:49.200620Z","shell.execute_reply":"2022-06-29T23:01:49.208491Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = RNN(dict_length, max_length, 10,5, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.210610Z","iopub.execute_input":"2022-06-29T23:01:49.210996Z","iopub.status.idle":"2022-06-29T23:01:49.233903Z","shell.execute_reply.started":"2022-06-29T23:01:49.210966Z","shell.execute_reply":"2022-06-29T23:01:49.232798Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def one_pass(model, dataloader, optimizer,lossFun, backwards=True, print_loss=False):\n    \n    if backwards == True:\n        model.train()\n    else:\n        model.eval()\n    \n    total_loss = 0.0\n    for x, x2, y in dataloader:\n        \n        y_pred = model(x, x2)\n#         y_pred = y_pred.detach().numpy()\n#         y = y.detach().numpy()\n        loss = lossFun(y_pred, y)\n        total_loss += loss.item()\n        \n        if backwards == True:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    avg_loss = total_loss / len(dataloader)\n    \n    if print_loss == True:\n        print(avg_loss)\n    \n    return avg_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.235032Z","iopub.execute_input":"2022-06-29T23:01:49.235663Z","iopub.status.idle":"2022-06-29T23:01:49.243299Z","shell.execute_reply.started":"2022-06-29T23:01:49.235627Z","shell.execute_reply":"2022-06-29T23:01:49.242452Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:49.244830Z","iopub.execute_input":"2022-06-29T23:01:49.245331Z","iopub.status.idle":"2022-06-29T23:01:59.672706Z","shell.execute_reply.started":"2022-06-29T23:01:49.245300Z","shell.execute_reply":"2022-06-29T23:01:59.671565Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"lossFun = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 5\n\nfor epoch in tqdm(range(num_epochs)):\n    print('Epoch: ', epoch)\n\n    train_loss = one_pass(model, train_dl, optimizer, lossFun, backwards=True, print_loss=False)\n\n    val_loss = one_pass(model, val_dl, optimizer, lossFun, backwards=False, print_loss=False)\n    print('Train Loss-%.4f Validation Loss-%.4f '%\n          (train_loss, val_loss))\n#     print('Val Loss-%.4f Val Accuracy-%.4f Val Precision-%.4f Val Recall-%.4f' %\n#           (val_loss, val_acc, val_precision, val_recall))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:01:59.674123Z","iopub.execute_input":"2022-06-29T23:01:59.674662Z","iopub.status.idle":"2022-06-29T23:11:52.002038Z","shell.execute_reply.started":"2022-06-29T23:01:59.674630Z","shell.execute_reply":"2022-06-29T23:11:52.000886Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Vanilla RNN using nn.RNN\nclass RNN_Dropout(nn.Module):\n    def __init__(self, dict_length, max_length, emb_size, hidden_size, output_size):\n        super(RNN_Dropout, self).__init__()\n        # embed the words\n        self.emb = nn.Embedding(dict_length, emb_size, padding_idx=0)  \n        \n        self.linear2 = nn.Linear((max_length*emb_size)+1, output_size)\n        self.dropout = nn.Dropout(p=.25)\n\n\n    def forward(self, x, x2):\n        \n        # RNN layer outputs a tuple, the output and the final hidden state\n        # taking the final hidden state as output\n#         print('-------', x.size(), max_length)\n        x = self.emb(x)\n        x = x.view(x.shape[0],x.shape[1]*x.shape[-1])\n\n        x2 = x2.unsqueeze(1)\n        X = torch.cat((x,x2), 1)\n        out = self.linear2(X)\n        out = self.dropout(out)\n        return out.squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:11:52.003606Z","iopub.execute_input":"2022-06-29T23:11:52.004720Z","iopub.status.idle":"2022-06-29T23:11:52.014856Z","shell.execute_reply.started":"2022-06-29T23:11:52.004677Z","shell.execute_reply":"2022-06-29T23:11:52.013706Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = RNN_Dropout(dict_length, max_length, 10,5, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:11:52.016255Z","iopub.execute_input":"2022-06-29T23:11:52.016917Z","iopub.status.idle":"2022-06-29T23:11:52.032514Z","shell.execute_reply.started":"2022-06-29T23:11:52.016870Z","shell.execute_reply":"2022-06-29T23:11:52.030932Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lossFun = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 5\n\nfor epoch in tqdm(range(num_epochs)):\n    print('Epoch: ', epoch)\n\n    train_loss = one_pass(model, train_dl, optimizer, lossFun, backwards=True, print_loss=False)\n\n    val_loss = one_pass(model, val_dl, optimizer, lossFun, backwards=False, print_loss=False)\n    print('Train Loss-%.4f Validation Loss-%.4f '%\n          (train_loss, val_loss))\n#     print('Val Loss-%.4f Val Accuracy-%.4f Val Precision-%.4f Val Recall-%.4f' %\n#           (val_loss, val_acc, val_precision, val_recall))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:11:52.034291Z","iopub.execute_input":"2022-06-29T23:11:52.034702Z","iopub.status.idle":"2022-06-29T23:20:59.073919Z","shell.execute_reply.started":"2022-06-29T23:11:52.034660Z","shell.execute_reply":"2022-06-29T23:20:59.072913Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = RNN(dict_length, max_length, 20,5, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:20:59.075306Z","iopub.execute_input":"2022-06-29T23:20:59.075717Z","iopub.status.idle":"2022-06-29T23:20:59.097773Z","shell.execute_reply.started":"2022-06-29T23:20:59.075686Z","shell.execute_reply":"2022-06-29T23:20:59.096839Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"lossFun = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 5\n\nfor epoch in tqdm(range(num_epochs)):\n    print('Epoch: ', epoch)\n\n    train_loss = one_pass(model, train_dl, optimizer, lossFun, backwards=True, print_loss=False)\n\n    val_loss = one_pass(model, val_dl, optimizer, lossFun, backwards=False, print_loss=False)\n    print('Train Loss-%.4f Validation Loss-%.4f '%\n          (train_loss, val_loss))\n#     print('Val Loss-%.4f Val Accuracy-%.4f Val Precision-%.4f Val Recall-%.4f' %\n#           (val_loss, val_acc, val_precision, val_recall))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:20:59.102058Z","iopub.execute_input":"2022-06-29T23:20:59.103091Z","iopub.status.idle":"2022-06-29T23:50:27.792529Z","shell.execute_reply.started":"2022-06-29T23:20:59.103048Z","shell.execute_reply":"2022-06-29T23:50:27.791460Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}